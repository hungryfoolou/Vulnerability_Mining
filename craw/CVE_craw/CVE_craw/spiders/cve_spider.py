# -*- coding: utf-8 -*-
import scrapy
from CVE_craw.items import CveCrawItem
import os


class CveSpiderSpider(scrapy.Spider):
    handle_httpstatus_list = [404]  # 不让scrapy跳过404
    name = 'cve_spider'
    allowed_domains = ['cve.mitre.org']
    start_urls = []

    id_path = os.getcwd() + '/data/cveid/'  # 存放cveid的文件夹
    id_files = os.listdir(id_path)  # cve_ref文件夹下的文件，只爬一个
    with open(id_path + id_files[0], 'r') as id_fr:
        try:
            lines = id_fr.readlines()  # 获取文件的所有行
            for line in lines:
                start_urls.append('https://cve.mitre.org/cgi-bin/cvename.cgi?name=' + line.replace('\n', ''))
        except Exception as e:
            print("error:" + e + "\n")

    print(1, '\n')
    log_path = os.getcwd() + '/log/'  # 存放log的文件夹
    log_name = log_path + 'error_ref.txt'  # 存放错误的ref
    with open(log_name, 'a', encoding='utf-8') as log_f:
        log_f.seek(0)  # 清空前先定位到要开始清空的位置
        log_f.truncate()  # 先清空
        log_f.write('404等错误状态码的参考链接:\n')

    # 记录可能包括s2r、eb的CVE编号文件列表
    cve_id_path = os.getcwd() + '/data/cveid/'  # 存放cveid的文件夹
    cve_id_filename = os.listdir(cve_id_path)[0]
    mays2r_path = os.getcwd() + '/data/s2r_info/'  # 存放s2r_info的文件夹
    mays2r_dir = 's2rinfo_' + cve_id_filename
    mays2r_file_path = os.path.join('%s\%s' % (mays2r_path, mays2r_dir))
    with open(mays2r_file_path, 'w', encoding='utf-8') as mays2rf:
        mays2rf.write('The list cveid not only include official cve info,so it may include other website\'s s2r:\n')

    # 解析bugsgentoo
    def parse_bugsgentoo(self, response):
        cve_craw_item = response.meta['item']
        if response.status == 404 or response.status == 403:
            yield cve_craw_item
        else:
            # link
            cve_craw_item['bugsgentoo_link'] = response.url
            # title
            title_section = ''  # 初始化，以免值为空时，len(NoneType)报错
            title_section = response.xpath('//*[@id="short_desc_nonedit_display"]/text()').extract_first()
            if title_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('bugsgentoo error ' + response.url)
            elif len(title_section) > 0:
                title0 = title_section.replace('\n', ' ')
                title1 = str(title0.encode('utf-8'))
                title2 = title1.strip("b' ")
                title3 = title2.strip("'")
                title4 = title3.strip()
                cve_craw_item['bugsgentoo_title'] = title4
            else:
                print('bugsgentoo error ' + response.url)
            # description
            content_section = ''
            content_section = response.xpath('string(//*[@id="c0"]/pre)').extract_first()
            if content_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('bugsgentoo error ' + response.url)
            elif len(content_section) > 0:
                cve_craw_item['bugsgentoo_description'] = content_section
            else:
                print('bugsgentoo error ' + response.url)
            yield cve_craw_item  # 返回数据到pipelines，进行数据的清洗和存储

    # 解析edb
    def parse_edb(self, response):
        cve_craw_item = response.meta['item']
        if response.status == 404 or response.status == 403:
            log_path = os.getcwd() + '/log/'  # 存放log的文件夹
            log_name = log_path + 'error_ref.txt'  # 存放错误的ref
            with open(log_name, 'a', encoding='utf-8') as log_f:
                log_f.write(response.url + '\n')
            yield cve_craw_item
        else:
            # link
            cve_craw_item['edb_link'] = response.url
            # title
            title_section = ''
            title_section = response.xpath('//h1[@class="card-title text-secondary text-center"]/text()').extract_first()
            if title_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('edb error ' + response.url)
            elif len(title_section) > 0:
                title0 = title_section.replace('\n', ' ').strip()
                title1 = str(title0.encode('utf-8'))
                title2 = title1.strip("b' ")
                title3 = title2.strip("'")
                title4 = title3.strip()
                cve_craw_item['edb_title'] = title4
            else:
                print('edb error ' + response.url)
            # description
            content_section = ''
            content_section = response.xpath('//code[starts-with(@class,"language")]/text()').extract_first()
            content_section1 = content_section.replace('\r\n', ' ')
            content_section2 = content_section1.replace('\\r\\n', ' ')
            if content_section2 is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('edb error ' + response.url)
            elif len(content_section2) > 0:
                cve_craw_item['edb_description'] = content_section2
            else:
                print('edb error ' + response.url)
            yield cve_craw_item  # 返回数据到pipelines，进行数据的清洗和存储

    # 解析marcinfo
    def parse_marcinfo(self, response):
        cve_craw_item = response.meta['item']
        if response.status == 404 or response.status == 403:
            yield cve_craw_item
        else:
            # link
            cve_craw_item['marcinfo_link'] = response.url
            # title
            title_section = ''
            title_section = response.xpath('string(/html/body/pre)').extract_first()
            if title_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('marcinfo error ' + response.url)
            elif len(title_section) > 0:
                if title_section.find('Subject:') != -1:
                    start_pos = title_section.find('Subject:') + 9
                    if title_section.find('From:') != -1:
                        end_pos = title_section.find('From:')
                        title_section = title_section[start_pos:end_pos]
                        title0 = title_section.replace('\n', ' ')
                        title1 = title0.strip()
                        cve_craw_item['marcinfo_title'] = title1
            else:
                print('marcinfo error ' + response.url)
            # description
            content_section = ''
            content_section = response.xpath('string(/html/body/pre)').extract_first()
            if content_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('marcinfo error ' + response.url)
            elif len(content_section) > 0:
                if content_section.find('Download RAW message or body') != -1:
                    start_pos = content_section.find('Download RAW message or body') + 30
                    end_pos = content_section[60:].find('[prev in list]')  # 找结束标志，但是网页前面也有[prev in list]，所以从第60个字符开始找
                    content_section = content_section[start_pos:end_pos + 60]  # 与上一行代码对应，也+60
                    # content_section = content_section.replace('\n', ' ')
                    content_section = content_section.strip()
                    cve_craw_item['marcinfo_description'] = content_section
            else:
                print('marcinfo error ' + response.url)
            yield cve_craw_item  # 返回数据到pipelines，进行数据的清洗和存储

    # 解析seclists
    def parse_seclists(self, response):
        cve_craw_item = response.meta['item']
        if response.status == 404 or response.status == 403:
            yield cve_craw_item
        else:
            # link
            cve_craw_item['seclists_link'] = response.url
            # title
            title_section = ''
            title_section = response.xpath('/html/head/title/text()').extract_first()
            if title_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('seclists error ' + response.url)
            elif len(title_section) > 0:
                title1 = title_section.strip()
                title2 = title1.replace('\t', ' ')
                cve_craw_item['seclists_title'] = title2
            else:
                print('seclists error ' + response.url)
            # description
            content_section = ''
            # https://seclists.org/oss-sec/2012/q1/77网页下除了pre，还有blockquote下也有内容
            content_xpath = response.xpath('(/html/body/table[2]/tr[1]/td[2]/table/tr/td/blockquote)|( /html/body/table[2]/tr[1]/td[2]/table/tr/td/pre/text())').extract()
            for i in content_xpath:
                content_section = content_section + i
            if content_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('seclists error ' + response.url)
            elif len(content_section) > 0:
                content_section = content_section.strip()
                end_pos = content_section.find('Sent through the Full') - 50
                cve_craw_item['seclists_description'] = content_section[0:end_pos]
            else:
                print('seclists error ' + response.url)
            yield cve_craw_item  # 返回数据到pipelines，进行数据的清洗和存储

    # 解析securesoftware，暂未测试该函数，目前cve的ref没有securesoftware
    def parse_securesoftware(self, response):
        cve_craw_item = response.meta['item']
        if response.status == 404 or response.status == 403:
            yield cve_craw_item
        else:
            # link
            cve_craw_item['securesoftware_link'] = response.url
            # title
            # 没有title
            # description
            content_section = ''
            content_section = response.xpath('/html/body/pre/text()').extract_first()
            if content_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('securesoftware error ' + response.url)
            elif len(content_section) > 0:
                content_section = content_section.strip()
                start_pos = content_section.find('Content-Type: text/plain; charset=us-ascii') + 75
                cve_craw_item['securesoftware_description'] = content_section[start_pos:]
            else:
                print('securesoftware error ' + response.url)
            yield cve_craw_item  # 返回数据到pipelines，进行数据的清洗和存储

    # 解析securityfocus
    def parse_securityfocus(self, response):
        cve_craw_item = response.meta['item']
        if response.status == 404 or response.status == 403:
            yield cve_craw_item
        else:
            # link
            cve_craw_item['securityfocus_link'] = response.url
            # title
            title_section = ''
            title_section = response.xpath('string(//*[@id="vulnerability"]/span)').extract_first()
            if title_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('securityfocus error ' + response.url)
            elif len(title_section) > 0:
                title0 = title_section.replace('\n', ' ')
                title1 = str(title0.encode('utf-8'))
                title2 = title1.strip("b' ")
                title3 = title2.strip("'")
                title4 = title3.strip()
                cve_craw_item['securityfocus_title'] = title4
            else:
                print('securityfocus_official error ' + response.url)
            # description
            content_section = ''
            content_section = response.xpath('string(//*[@id="vulnerability"])').extract_first()
            content_section1 = content_section.replace(title_section, '')  # 由于content无需包括标题部分，所以去除标题部分
            content_section2 = content_section1.replace('\n\t', '')
            if content_section2 is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('securityfocus error ' + response.url)
            elif len(content_section2) > 0:
                cve_craw_item['securityfocus_description'] = content_section2
            else:
                print('securityfocus_official error ' + response.url)
            yield cve_craw_item  # 返回数据到pipelines，进行数据的清洗和存储

    # 解析securitytracker
    def parse_securitytracker(self, response):
        cve_craw_item = response.meta['item']
        if response.status == 404 or response.status == 403:
            yield cve_craw_item
        else:
            # link
            cve_craw_item['securitytracker_link'] = response.url
            # title
            title_section = ''
            title_section = response.xpath('/html/head/title/text()').extract_first()
            if title_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断，排除网页https://securitytracker.com/id?017459
                print('securitytracker error ' + response.url)
            elif len(title_section) > 0:
                title_section = title_section.rstrip(' - SecurityTracker')  # 避免title_section为空的情况下进行rstrip()报错
                title0 = title_section.replace('\n', ' ')
                title1 = str(title0.encode('utf-8'))
                title2 = title1.strip("b' ")
                title3 = title2.strip("'")
                title4 = title3.strip()
                cve_craw_item['securitytracker_title'] = title4
            else:
                print('securitytracker error ' + response.url)
            # description
            content_section = ''
            content_section = response.xpath('string(/html/body)').extract_first()
            start_loc = content_section.find('Description:') + 12
            end_loc1 = content_section.find('Impact:')
            end_loc2 = (end_loc1 + 6) + content_section[end_loc1+6:].find('Impact:')  # 找到第2个Impact
            content_section1 = ''  # 判断是否为Nonetype之前需要先初始化
            if start_loc < end_loc2 and start_loc != -1 and end_loc2 != -1:
                content_section1 = content_section[start_loc:end_loc2]
            if content_section1 is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('securitytracker error ' + response.url)
            elif len(content_section1) > 0:
                cve_craw_item['securitytracker_description'] = content_section1
            else:
                print('securitytracker error ' + response.url)
            yield cve_craw_item  # 返回数据到pipelines，进行数据的清洗和存储

    # 解析sourceware
    def parse_sourceware(self, response):
        cve_craw_item = response.meta['item']
        if response.status == 404 or response.status == 403:
            yield cve_craw_item
        else:
            # link
            cve_craw_item['sourceware_link'] = response.url
            # title
            title_section = ''
            title_section = response.xpath('string(//*[@id="short_desc_nonedit_display"])').extract_first()
            if title_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('sourceware error ' + response.url)
            elif len(title_section) > 0:
                title0 = title_section.replace('\n', ' ')
                title1 = str(title0.encode('utf-8'))
                title2 = title1.strip("b' ")
                title3 = title2.strip("'")
                title4 = title3.strip()
                cve_craw_item['sourceware_title'] = title4
            else:
                print('sourceware error ' + response.url)
            # description
            content_section = ''
            content_section = response.xpath('string(//*[@id="c0"]/pre)').extract_first()
            if content_section is None:  # 变量类型如果为Nonetype则调用len()出错，所以单独判断
                print('sourceware error ' + response.url)
            elif len(content_section) > 0:
                cve_craw_item['sourceware_description'] = content_section
            else:
                print('sourceware error ' + response.url)
            yield cve_craw_item  # 返回数据到pipelines，进行数据的清洗和存储

    # 主解析函数
    def parse(self, response):
        description = response.xpath('string(//*[@id="GeneratedTable"]/table/tr[4]/td)').extract_first()
        cve_craw_item = CveCrawItem()
        for key in cve_craw_item.fields:  # 在其他解析函数中用到其他key，现在这里统一初始化
            cve_craw_item[key] = ''
        cve_craw_item['cve_link'] = response.url
        cve_craw_item['cve_id'] = response.url[response.url.find('name=',)+5:]
        cve_craw_item['cve_description'] = description.replace('\n', ' ').strip()

        # 获取参考链接
        refs = ''
        refs = response.xpath('//*[@id="GeneratedTable"]/table/tr[7]/td/ul/li/a[@href]/text()').extract()
        i = 0
        while i < len(refs):
            if refs[i].find('http') == -1:
                refs.remove(refs[i])
            else:
                refs[i] = refs[i][refs[i].find('http'):]
            i = i+1

        flag_ref = 0
        for link in refs:
            http_pos = link.find('http')
            if (http_pos != -1) and (link.find('bugs.gentoo.org/show_bug.cgi') != -1):
                flag_ref = 1
                link = link[http_pos:]
                yield scrapy.Request(link, callback=self.parse_bugsgentoo, meta={'item': cve_craw_item}, dont_filter=True)  # 设置dont_filter是因为不在allowed_domains里
            elif (http_pos != -1) and (link.find('exploit-db.com') != -1):
                flag_ref = 1
                link = link[http_pos:]  # 避免"url:https://www.exploit-db.com/exploits/4219"
                yield scrapy.Request(link, callback=self.parse_edb, meta={'item': cve_craw_item}, dont_filter=True)
            elif (http_pos != -1) and (link.find('marc.info') != -1):
                flag_ref = 1
                link = link[http_pos:]
                yield scrapy.Request(link, callback=self.parse_marcinfo, meta={'item': cve_craw_item}, dont_filter=True)
            elif (http_pos != -1) and (link.find('seclists.org') != -1):
                flag_ref = 1
                link = link[http_pos:]
                yield scrapy.Request(link, callback=self.parse_seclists, meta={'item': cve_craw_item}, dont_filter=True)
            elif link.find('securesoftware') != -1:  # securesoftware没有以http开头
                flag_ref = 1
                yield scrapy.Request(link, callback=self.parse_securesoftware, meta={'item': cve_craw_item}, dont_filter=True)
            elif (http_pos != -1) and (link.find('securityfocus.com/bid') != -1):
                flag_ref = 1
                link = link[http_pos:]
                if link.find('/exploit') == -1:  # 直接爬取/exploit页面
                    link = link.rstrip('/')  # 有的链接右边末尾有/
                    link = link + '/exploit'
                yield scrapy.Request(link, callback=self.parse_securityfocus, meta={'item': cve_craw_item}, dont_filter=True)
            elif (http_pos != -1) and (link.find('securitytracker.com/id') != -1):  # 有的securitytracker链接没有www
                flag_ref = 1
                link = link[http_pos:]
                yield scrapy.Request(link, callback=self.parse_securitytracker, meta={'item': cve_craw_item}, dont_filter=True)
            elif (http_pos != -1) and (link.find('sourceware.org/bugzilla/show_bug.cgi') != -1):
                flag_ref = 1
                link = link[http_pos:]
                yield scrapy.Request(link, callback=self.parse_sourceware, meta={'item': cve_craw_item}, dont_filter=True)

        if flag_ref == 0:  # cve连一个满足需要的ref都没有，返回cve基本信息
            yield cve_craw_item

