# -*- coding: utf-8 -*-
import scrapy
from cveid_craw.items import CveidCrawItem
import os


class CveidSpiderSpider(scrapy.Spider):
    name = 'cveid_spider'
    allowed_domains = ['www.cvedetails.com']
    start_urls = []
    cveid_start_url_path = os.getcwd() + '/data/cveid_start_url/'  # 存放cveid_start_url的文件夹
    cveid_start_url_files = os.listdir(cveid_start_url_path)  # cve_id文件夹下的文件，只有一个
    with open(cveid_start_url_path + cveid_start_url_files[0], 'r') as cveid_start_url_fr:
        try:
            lines = cveid_start_url_fr.readlines()  # 获取文件的所有行
            for line in lines:
                start_urls.append(line.replace('\n', ''))
        except Exception as e:
            print("error:" + e + "\n")

    # 清空cveid内容
    id_path = os.getcwd() + '/data/cveid_by_kind/'  # 存放cve_id的文件夹
    id_files = os.listdir(id_path)  # cve_id文件夹下的文件，如果有文件，把这个唯一的一个文件初始化为空
    if len(id_files) > 0:
        with open(id_path + id_files[0], 'a', encoding='utf-8') as id_f:
            id_f.seek(0)  # 清空前先定位到要开始清空的位置
            id_f.truncate()  # 先清空

    def parse(self, response):
        cveid_list = response.xpath('//*[@id="vulnslisttable"]/tr[@class="srrowns"]/td[@nowrap]/a/text()').extract()
        cveid_item = CveidCrawItem()
        cveid_item['id'] = cveid_list

        # 获取当前pagenum
        pagenum_start_pos = response.url.find('page=')
        pagenum_tmp_link = response.url[pagenum_start_pos+5:]
        pagenum_end_pos = pagenum_tmp_link.find('&')
        pagenum = pagenum_tmp_link[:pagenum_end_pos]
        pagenum = int(pagenum)

        # 保存cve类别，方便后面保存cveid内容时放在对应文件中，比如类别为dos的所有cveid放在dos文件中
        kind_dict = {'opdos':'dos', 'opec':'execution','opov':'overflow', 'opcsrf': 'csrf' , 'opgpriv' : 'gainpre','opsqli' : 'sqli' , 'opxss' : 'xss' , 'opdirt' : 'dirtra' , 'opmemc' : 'memc' , 'ophttprs' : 'httprs' , 'opbyp' : 'bypass' , 'opfileinc' : 'fileinc' , 'opginf' : 'infor'}
        kind_tmp_link = pagenum_tmp_link[pagenum_end_pos+1:]
        end_pos = kind_tmp_link.find('=1')
        tmp_link = kind_tmp_link[:end_pos]
        start_pos = tmp_link.rfind('&')  # 从右边开始找
        tmp_kind = tmp_link[start_pos + 1:]
        cve_kind = kind_dict[tmp_kind]
        cveid_item['kind'] = cve_kind

        yield cveid_item  # 返回数据到pipelines，进行数据的清洗和存储

        max_pagenum = response.xpath('//*[@id="pagingb"]/a[last()]/text()').extract_first()  # 最大页面数量
        max_pagenum = int(max_pagenum)

        # 解析下一页
        next_pagenum = pagenum + 1
        if next_pagenum <= max_pagenum:  # 测试使用小数字替代max_pagenum，真实运行需要修改为max_pagenum
            current_page = 'page=' + str(pagenum)
            next_page = 'page=' + str(next_pagenum)
            next_link = response.url.replace(current_page, next_page)
            yield scrapy.Request(next_link, callback=self.parse)  # 不用加参数dont_filter=True


